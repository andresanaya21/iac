# this file install infra in aws (sgs, route-table, ec2, lb)
# install 1 master (control plane), 1 node (worker) using an existent infra (vcp, igw, natgtw)
# https://blog.scottlowe.org/2019/09/09/consuming-preexisting-aws-infrastructure-with-cluster-api/
# https://techbloc.net/archives/4661
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: capi-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: capi-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSCluster
    name: capi-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: capi-cluster
  namespace: default
spec:
  region: eu-west-3
  sshKeyName: discovery-ec2
  bastion:
    enabled: true
  network:
    vpc:
      id: vpc-0e9fbbd63a090909e
    subnets:
      - id: subnet-0917c1b0a3b3372e6  #subnet private
      - id: subnet-06c9e1971e32a7319  #subnet private
      - id: subnet-0e0fa156b6e98194d  #subnet private
      - id: subnet-07a803679ba7f87bc  #subnet public
      - id: subnet-0e4b929972f4d394b  #subnet public
      - id: subnet-01ffc32056b5c5789  #subnet public
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: capi-cluster-control-plane
  namespace: default
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: aws:///'{{ ds.meta_data.instance_id }}'
        name: '{{ ds.meta_data.local_hostname }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: aws:///'{{ ds.meta_data.instance_id }}'
        name: '{{ ds.meta_data.local_hostname }}'
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      kind: AWSMachineTemplate
      name: capi-cluster-control-plane
  replicas: 1
  version: v1.27.3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: capi-cluster-control-plane
  namespace: default
spec:
  template:
    spec:
      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io
      instanceType: t3.large
      sshKeyName: discovery-ec2
      rootVolume:
        size: 80
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: capi-cluster-md-0
  namespace: default
spec:
  clusterName: capi-cluster
  replicas: 1
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: capi-cluster-md-0
      clusterName: capi-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: capi-cluster-md-0
      version: v1.27.3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: capi-cluster-md-0
  namespace: default
spec:
  template:
    spec:
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceType: t3.large
      sshKeyName: discovery-ec2
      rootVolume:
        size: 80
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: capi-cluster-md-0
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: aws:///'{{ ds.meta_data.instance_id }}'
          name: '{{ ds.meta_data.local_hostname }}'